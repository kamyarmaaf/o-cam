// hooks/useFaceDetectionFixed.ts - Fixed version for reference face capture
import { useRef, useEffect, useState, useCallback } from 'react';
import * as faceapi from 'face-api.js';
import * as tf from '@tensorflow/tfjs';
import '@tensorflow/tfjs-backend-webgl';

export type ViolationType = 'multiple_faces' | 'face_mismatch' | 'no_face';

export interface Violation {
  timestamp: Date;
  faceCount: number;
  screenshot: string;
  type: ViolationType;
  distance?: number;
}

export interface UseFaceDetectionOptions {
  detectFps?: number;
  threshold?: number;
  drawOverlay?: boolean;
  drawLandmarks?: boolean;
}

export interface UseFaceDetectionReturn {
  videoRef: React.RefObject<HTMLVideoElement>;
  canvasRef: React.RefObject<HTMLCanvasElement>;
  overlayCanvasRef: React.RefObject<HTMLCanvasElement>;

  isInitialized: boolean;
  currentFaceCount: number;
  isViolating: boolean;
  violations: Violation[];
  referenceFace: Float32Array | null;
  isModelsLoaded: boolean;
  isMonitoring: boolean;
  cameraError: boolean;

  lastDistance?: number;
  fps?: number;

  startCamera: () => Promise<void>;
  stopCamera: () => void;
  captureReferenceFace: () => Promise<boolean>;
  startMonitoring: () => void;
}

type DetectionResult = {
  faceCount: number;
  violationType: ViolationType | null;
  distance?: number;
};

export const useFaceDetectionFixed = (
  enabled: boolean = false,
  options: UseFaceDetectionOptions = {}
): UseFaceDetectionReturn => {
  const {
    detectFps = 8,
    threshold = 0.6,
    drawOverlay = true,
    drawLandmarks = false
  } = options;

  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const overlayCanvasRef = useRef<HTMLCanvasElement>(null);

  const streamRef = useRef<MediaStream | null>(null);
  const rafIdRef = useRef<number | null>(null);
  const detectionInProgressRef = useRef<boolean>(false);

  const [isInitialized, setIsInitialized] = useState(false);
  const [currentFaceCount, setCurrentFaceCount] = useState(0);
  const [isViolating, setIsViolating] = useState(false);
  const [violations, setViolations] = useState<Violation[]>([]);
  const [referenceFace, setReferenceFace] = useState<Float32Array | null>(null);
  const [isModelsLoaded, setIsModelsLoaded] = useState(false);
  const [isMonitoring, setIsMonitoring] = useState(false);
  const [cameraError, setCameraError] = useState(false);

  const [lastDistance, setLastDistance] = useState<number | undefined>(undefined);
  const [fps, setFps] = useState<number | undefined>(undefined);

  const referenceFaceRef = useRef<Float32Array | null>(null);
  useEffect(() => { referenceFaceRef.current = referenceFace; }, [referenceFace]);

  const isMonitoringRef = useRef<boolean>(false);
  useEffect(() => { isMonitoringRef.current = isMonitoring; }, [isMonitoring]);

  const addLog = (message: string) => {
    console.log(`[FaceDetectionFixed] ${message}`);
  };

  useEffect(() => {
    (async () => {
      try {
        addLog('Ø´Ø±ÙˆØ¹ ØªÙ†Ø¸ÛŒÙ… TensorFlow.js backend...');
        await tf.setBackend('webgl');
        await tf.ready();
        addLog(`âœ… TensorFlow.js backend ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯: ${tf.getBackend()}`);
      } catch (e) {
        addLog('âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙ†Ø¸ÛŒÙ… WebGL backendØŒ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ CPU backend...');
        try {
          await tf.setBackend('cpu');
          await tf.ready();
          addLog(`âœ… TensorFlow.js CPU backend ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯: ${tf.getBackend()}`);
        } catch (cpuError) {
          addLog('âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙ†Ø¸ÛŒÙ… CPU backend: ' + cpuError);
        }
      }
    })();
  }, []);

  const loadModels = useCallback(async () => {
    try {
      addLog('Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ face-api.js...');
      setIsModelsLoaded(false);
      
      await Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri('/models'),
        faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
        faceapi.nets.faceRecognitionNet.loadFromUri('/models'),
      ]);
      setIsModelsLoaded(true);
      addLog('âœ… face-api models loaded from /models');
    } catch (err) {
      addLog('âŒ Local model load failed: ' + err);
      addLog('Trying CDN fallback...');
      try {
        const base = 'https://justadudewhohacks.github.io/face-api.js/models';
        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri(base),
          faceapi.nets.faceLandmark68Net.loadFromUri(base),
          faceapi.nets.faceRecognitionNet.loadFromUri(base),
        ]);
        setIsModelsLoaded(true);
        addLog('âœ… face-api models loaded from CDN');
      } catch (cdnErr) {
        addLog('âŒ CDN model load failed: ' + cdnErr);
        setIsModelsLoaded(false);
      }
    }
  }, []);

  const waitForVideoReady = useCallback(async (timeoutMs = 10000): Promise<boolean> => {
    const video = videoRef.current;
    const stream = streamRef.current;
    if (!video || !stream) {
      addLog('âŒ ÙˆÛŒØ¯ÛŒÙˆ ÛŒØ§ Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯');
      return false;
    }

    (video as any).playsInline = true;
    video.muted = true;

    const isReady = () => {
      const ready = video.readyState >= 2 && 
                   video.videoWidth > 0 && 
                   video.videoHeight > 0 &&
                   !video.paused &&
                   !video.ended;
      return ready;
    };

    addLog('Ø¯Ø± Ø­Ø§Ù„ Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù† ÙˆÛŒØ¯ÛŒÙˆ...');

    try { 
      await video.play(); 
      addLog('âœ… video.play() Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯');
    } catch (e) {
      addLog('âš ï¸ video.play() Ø®Ø·Ø§ Ø¯Ø§Ø¯: ' + e);
    }

    if (isReady()) {
      addLog('âœ… ÙˆÛŒØ¯ÛŒÙˆ Ø¨Ù„Ø§ÙØ§ØµÙ„Ù‡ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨ÙˆØ¯');
      return true;
    }

    return await new Promise<boolean>((resolve) => {
      let settled = false;
      let attempts = 0;
      const maxAttempts = 50;

      const checkReady = () => {
        if (settled) return;
        attempts++;
        
        if (isReady()) {
          addLog(`âœ… ÙˆÛŒØ¯ÛŒÙˆ Ø¨Ø¹Ø¯ Ø§Ø² ${attempts} ØªÙ„Ø§Ø´ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯`);
          settled = true;
          resolve(true);
          return;
        }

        if (attempts >= maxAttempts) {
          addLog(`âŒ ÙˆÛŒØ¯ÛŒÙˆ Ø¨Ø¹Ø¯ Ø§Ø² ${attempts} ØªÙ„Ø§Ø´ Ø¢Ù…Ø§Ø¯Ù‡ Ù†Ø´Ø¯`);
          settled = true;
          resolve(false);
          return;
        }

        setTimeout(checkReady, 100);
      };

      setTimeout(checkReady, 100);
    });
  }, []);

  const drawFrameToCanvas = useCallback(() => {
    const video = videoRef.current;
    const canvas = canvasRef.current;
    if (!video || !canvas) return;
    const w = video.videoWidth;
    const h = video.videoHeight;
    if (!w || !h) return;

    if (canvas.width !== w || canvas.height !== h) {
      canvas.width = w;
      canvas.height = h;
    }
    const ctx = canvas.getContext('2d');
    if (!ctx) return;

    ctx.save();
    ctx.scale(-1, 1);
    ctx.drawImage(video, -w, 0, w, h);
    ctx.restore();
  }, []);

  const drawOverlayBoxes = useCallback((detections: any[]) => {
    if (!drawOverlay) return;

    const video = videoRef.current;
    const overlay = overlayCanvasRef.current;
    if (!video || !overlay) return;

    const w = video.videoWidth;
    const h = video.videoHeight;
    if (!w || !h) return;

    if (overlay.width !== w || overlay.height !== h) {
      overlay.width = w;
      overlay.height = h;
    }
    const ctx = overlay.getContext('2d');
    if (!ctx) return;

    ctx.clearRect(0, 0, overlay.width, overlay.height);

    const resized = faceapi.resizeResults(detections, { width: overlay.width, height: overlay.height });
    const draw = (faceapi as any).draw;

    if (draw?.drawDetections) {
      draw.drawDetections(overlay, resized);
    } else {
      resized.forEach((d: any) => {
        const { x, y, width, height } = d.detection.box;
        ctx.strokeStyle = 'rgba(0,255,0,0.9)';
        ctx.lineWidth = 2;
        ctx.strokeRect(x, y, width, height);
      });
    }

    if (drawLandmarks && draw?.drawFaceLandmarks) {
      draw.drawFaceLandmarks(overlay, resized);
    }
  }, [drawOverlay, drawLandmarks]);

  const detectOnce = useCallback(async (): Promise<DetectionResult> => {
    const video = videoRef.current;
    
    if (!video || !isModelsLoaded || video.videoWidth === 0 || video.videoHeight === 0) {
      addLog('âŒ ÙˆÛŒØ¯ÛŒÙˆ ÛŒØ§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³ØªÙ†Ø¯');
      return { faceCount: 0, violationType: null, distance: undefined };
    }

    if (video.readyState < 2) {
      addLog('âŒ ÙˆÛŒØ¯ÛŒÙˆ Ù‡Ù†ÙˆØ² Ø¢Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³Øª (readyState: ' + video.readyState + ')');
      return { faceCount: 0, violationType: null, distance: undefined };
    }
    
    if (video.videoWidth === 0 || video.videoHeight === 0) {
      addLog('âŒ Ø§Ø¨Ø¹Ø§Ø¯ ÙˆÛŒØ¯ÛŒÙˆ ØµÙØ± Ø§Ø³Øª');
      return { faceCount: 0, violationType: null, distance: undefined };
    }

    if (video.paused || video.ended) {
      addLog('âŒ ÙˆÛŒØ¯ÛŒÙˆ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø®Ø´ Ù†ÛŒØ³Øª');
      return { faceCount: 0, violationType: null, distance: undefined };
    }

    try {
      addLog('Ø¯Ø± Ø­Ø§Ù„ ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡...');
      
      await new Promise(resolve => setTimeout(resolve, 100));
      
      const options = new faceapi.TinyFaceDetectorOptions({ 
        inputSize: 320, 
        scoreThreshold: 0.3 
      });
      
      const results = await faceapi
        .detectAllFaces(video, options)
        .withFaceLandmarks()
        .withFaceDescriptors();

      drawOverlayBoxes(results);
      addLog(`âœ… ${results.length} Ú†Ù‡Ø±Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯`);

      const faceCount = results.length;

      if (faceCount > 1) {
        setLastDistance(undefined);
        addLog(`ğŸš¨ ØªØ®Ù„Ù: Ú†Ù†Ø¯ Ú†Ù‡Ø±Ù‡ (${faceCount})`);
        return { faceCount, violationType: 'multiple_faces', distance: undefined };
      }

      if (faceCount === 0) {
        setLastDistance(undefined);
        addLog('âš ï¸ Ù‡ÛŒÚ† Ú†Ù‡Ø±Ù‡â€ŒØ§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯');
        return { faceCount: 0, violationType: 'no_face', distance: undefined };
      }

      const ref = referenceFaceRef.current;
      if (ref && results[0]?.descriptor) {
        const dist = faceapi.euclideanDistance(ref, results[0].descriptor);
        setLastDistance(dist);
        addLog(`ÙØ§ØµÙ„Ù‡ Ú†Ù‡Ø±Ù‡: ${dist.toFixed(3)}`);
        if (dist > threshold) {
          addLog(`ğŸš¨ ØªØ®Ù„Ù: Ø¹Ø¯Ù… ØªØ·Ø§Ø¨Ù‚ Ú†Ù‡Ø±Ù‡ (ÙØ§ØµÙ„Ù‡: ${dist.toFixed(3)})`);
          return { faceCount: 1, violationType: 'face_mismatch', distance: dist };
        }
        addLog('âœ… Ú†Ù‡Ø±Ù‡ ØªØ·Ø§Ø¨Ù‚ Ø¯Ø§Ø±Ø¯');
        return { faceCount: 1, violationType: null, distance: dist };
      }

      setLastDistance(undefined);
      addLog('âœ… ÛŒÚ© Ú†Ù‡Ø±Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯ (Ø¨Ø¯ÙˆÙ† Ù…Ø±Ø¬Ø¹)');
      return { faceCount: 1, violationType: null, distance: undefined };
    } catch (error) {
      addLog('âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡: ' + error);
      return { faceCount: 0, violationType: null, distance: undefined };
    }
  }, [isModelsLoaded, threshold, drawOverlayBoxes]);

  const captureScreenshot = useCallback((): string => {
    const canvas = canvasRef.current;
    if (!canvas) return '';
    drawFrameToCanvas();
    return canvas.toDataURL('image/jpeg', 0.8);
  }, [drawFrameToCanvas]);

  // FIXED VERSION - Much more robust reference face capture
  const captureReferenceFace = useCallback(async (): Promise<boolean> => {
    const video = videoRef.current;
    if (!video || !isModelsLoaded) {
      addLog('âŒ ÙˆÛŒØ¯ÛŒÙˆ ÛŒØ§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø«Ø¨Øª Ú†Ù‡Ø±Ù‡ Ù…Ø±Ø¬Ø¹ Ø¢Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³ØªÙ†Ø¯');
      return false;
    }

    addLog('Ø¯Ø± Ø­Ø§Ù„ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø«Ø¨Øª Ú†Ù‡Ø±Ù‡ Ù…Ø±Ø¬Ø¹...');

    if (video.readyState < 2 || video.videoWidth === 0 || video.videoHeight === 0) {
      addLog('âŒ ÙˆÛŒØ¯ÛŒÙˆ Ø¨Ø±Ø§ÛŒ Ø«Ø¨Øª Ú†Ù‡Ø±Ù‡ Ù…Ø±Ø¬Ø¹ Ø¢Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³Øª');
      addLog(`ÙˆØ¶Ø¹ÛŒØª: readyState=${video.readyState}, width=${video.videoWidth}, height=${video.videoHeight}`);
      
      addLog('Ø¯Ø± Ø­Ø§Ù„ Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù† ÙˆÛŒØ¯ÛŒÙˆ...');
      const ready = await waitForVideoReady(5000);
      if (!ready) {
        addLog('âŒ ÙˆÛŒØ¯ÛŒÙˆ Ù¾Ø³ Ø§Ø² Ø§Ù†ØªØ¸Ø§Ø± Ù‡Ù… Ø¢Ù…Ø§Ø¯Ù‡ Ù†Ø´Ø¯');
        return false;
      }
    }

    try {
      await new Promise(resolve => setTimeout(resolve, 300));
      
      addLog('Ø¯Ø± Ø­Ø§Ù„ ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø¨Ø±Ø§ÛŒ Ø«Ø¨Øª Ù…Ø±Ø¬Ø¹...');
      
      // Try multiple approaches for better detection
      let detection = null;
      let allDetections = [];
      
      // Approach 1: Try with very low threshold
      try {
        const options1 = new faceapi.TinyFaceDetectorOptions({ 
          inputSize: 320, 
          scoreThreshold: 0.15 // Very low threshold
        });
        
        allDetections = await faceapi
          .detectAllFaces(video, options1)
          .withFaceLandmarks()
          .withFaceDescriptors();
        
        addLog(`ØªØ¹Ø¯Ø§Ø¯ Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ (Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø³ÛŒØ§Ø± Ù¾Ø§ÛŒÛŒÙ†): ${allDetections.length}`);
        
        if (allDetections.length === 1) {
          detection = allDetections[0];
          addLog(`âœ… ÛŒÚ© Ú†Ù‡Ø±Ù‡ Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² ${detection.detection.score.toFixed(3)} Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯`);
        } else if (allDetections.length > 1) {
          // If multiple faces, take the one with highest confidence
          detection = allDetections.reduce((prev, current) => 
            prev.detection.score > current.detection.score ? prev : current
          );
          addLog(`âœ… Ø¨Ù‡ØªØ±ÛŒÙ† Ú†Ù‡Ø±Ù‡ Ø§Ø² Ø¨ÛŒÙ† ${allDetections.length} Ú†Ù‡Ø±Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯ (Ø§Ù…ØªÛŒØ§Ø²: ${detection.detection.score.toFixed(3)})`);
        }
      } catch (e1) {
        addLog('âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø¨Ø§ Ø¢Ø³ØªØ§Ù†Ù‡ Ù¾Ø§ÛŒÛŒÙ†: ' + e1);
      }

      // Approach 2: If no detection, try with larger input size
      if (!detection) {
        try {
          addLog('Ø¯Ø± Ø­Ø§Ù„ ØªÙ„Ø§Ø´ Ø¨Ø§ Ø³Ø§ÛŒØ² ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø²Ø±Ú¯ØªØ±...');
          const options2 = new faceapi.TinyFaceDetectorOptions({ 
            inputSize: 416, 
            scoreThreshold: 0.2
          });
          
          allDetections = await faceapi
            .detectAllFaces(video, options2)
            .withFaceLandmarks()
            .withFaceDescriptors();
          
          addLog(`ØªØ¹Ø¯Ø§Ø¯ Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ (Ø³Ø§ÛŒØ² Ø¨Ø²Ø±Ú¯ØªØ±): ${allDetections.length}`);
          
          if (allDetections.length > 0) {
            detection = allDetections[0];
            addLog(`âœ… Ú†Ù‡Ø±Ù‡ Ø¨Ø§ Ø³Ø§ÛŒØ² Ø¨Ø²Ø±Ú¯ØªØ± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯ (Ø§Ù…ØªÛŒØ§Ø²: ${detection.detection.score.toFixed(3)})`);
          }
        } catch (e2) {
          addLog('âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø¨Ø§ Ø³Ø§ÛŒØ² Ø¨Ø²Ø±Ú¯ØªØ±: ' + e2);
        }
      }

      // Approach 3: Last resort - try different settings
      if (!detection) {
        try {
          addLog('Ø¯Ø± Ø­Ø§Ù„ ØªÙ„Ø§Ø´ Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†...');
          const options3 = new faceapi.TinyFaceDetectorOptions({ 
            inputSize: 320, 
            scoreThreshold: 0.1 // Extremely low threshold
          });
          
          const basicDetections = await faceapi.detectAllFaces(video, options3);
          addLog(`ØªØ¹Ø¯Ø§Ø¯ Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡: ${basicDetections.length}`);
          
          if (basicDetections.length > 0) {
            // Try to get full detection for the first face
            const optionsFull = new faceapi.TinyFaceDetectorOptions({ 
              inputSize: 320, 
              scoreThreshold: 0.1
            });
            
            const fullDetections = await faceapi
              .detectAllFaces(video, optionsFull)
              .withFaceLandmarks()
              .withFaceDescriptors();
            
            if (fullDetections.length > 0) {
              detection = fullDetections[0];
              addLog(`âœ… Ú†Ù‡Ø±Ù‡ Ø¨Ø§ Ø±ÙˆØ´ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯ (Ø§Ù…ØªÛŒØ§Ø²: ${detection.detection.score.toFixed(3)})`);
            }
          }
        } catch (e3) {
          addLog('âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø¨Ø§ Ø±ÙˆØ´ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†: ' + e3);
        }
      }

      if (detection?.descriptor) {
        setReferenceFace(detection.descriptor);
        referenceFaceRef.current = detection.descriptor;
        addLog('âœ… Ú†Ù‡Ø±Ù‡ Ù…Ø±Ø¬Ø¹ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø«Ø¨Øª Ø´Ø¯');
        addLog(`Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯ÛŒØ³Ú©Ø±ÛŒÙ¾ØªÙˆØ±: ${detection.descriptor.length}`);
        addLog(`Ø§Ù…ØªÛŒØ§Ø² ØªØ´Ø®ÛŒØµ: ${detection.detection.score.toFixed(3)}`);
        return true;
      } else {
        addLog('âŒ Ù‡ÛŒÚ† Ú†Ù‡Ø±Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø«Ø¨Øª Ù…Ø±Ø¬Ø¹ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯');
        
        if (allDetections.length === 0) {
          addLog('Ù‡ÛŒÚ† Ú†Ù‡Ø±Ù‡â€ŒØ§ÛŒ Ø¯Ø± ØªØµÙˆÛŒØ± Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯');
          addLog('Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ:');
          addLog('- Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ú†Ù‡Ø±Ù‡ Ø´Ù…Ø§ Ø¯Ø± Ø¯ÙˆØ±Ø¨ÛŒÙ† Ù‚Ø§Ø¨Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø§Ø³Øª');
          addLog('- Ù†ÙˆØ± Ù…Ø­ÛŒØ· Ø±Ø§ Ù…Ù†Ø§Ø³Ø¨ Ú©Ù†ÛŒØ¯');
          addLog('- ØµÙˆØ±Øª Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒØ¯');
          addLog('- ÙØ§ØµÙ„Ù‡ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø² Ø¯ÙˆØ±Ø¨ÛŒÙ† ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯ (Ø­Ø¯ÙˆØ¯ 50-100 Ø³Ø§Ù†ØªÛŒâ€ŒÙ…ØªØ±)');
        } else if (allDetections.length > 1) {
          addLog(`Ú†Ù†Ø¯ÛŒÙ† Ú†Ù‡Ø±Ù‡ (${allDetections.length}) Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯`);
          addLog('Ù„Ø·ÙØ§Ù‹ ÙÙ‚Ø· ÛŒÚ© Ù†ÙØ± Ø¯Ø± ØªØµÙˆÛŒØ± Ø¨Ø§Ø´Ø¯');
        }
        
        return false;
      }
    } catch (error) {
      addLog('âŒ Ø®Ø·Ø§ Ø¯Ø± Ø«Ø¨Øª Ú†Ù‡Ø±Ù‡ Ù…Ø±Ø¬Ø¹: ' + error);
      return false;
    }
  }, [isModelsLoaded, waitForVideoReady]);

  const startMonitoring = useCallback(() => {
    if (!isModelsLoaded || !referenceFaceRef.current) return;
    setIsMonitoring(true);
  }, [isModelsLoaded]);

  const startCamera = useCallback(async () => {
    try {
      addLog('Ø´Ø±ÙˆØ¹ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÙˆØ±Ø¨ÛŒÙ†...');
      setCameraError(false);

      if (!isModelsLoaded) {
        addLog('Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ...');
        await loadModels();
      }

      if (!streamRef.current) {
        addLog('Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ†...');
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { 
            width: { ideal: 640 }, 
            height: { ideal: 480 }, 
            facingMode: 'user' 
          },
          audio: false,
        });
        streamRef.current = stream;
        addLog('âœ… Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯');
      }

      if (videoRef.current) {
        (videoRef.current as any).playsInline = true;
        videoRef.current.muted = true;
        videoRef.current.autoplay = true;

        if (videoRef.current.srcObject) {
          const oldStream = videoRef.current.srcObject as MediaStream;
          oldStream.getTracks().forEach(track => track.stop());
        }

        videoRef.current.srcObject = streamRef.current;
        addLog('Ø¯Ø± Ø­Ø§Ù„ ØªÙ†Ø¸ÛŒÙ… ÙˆÛŒØ¯ÛŒÙˆ...');
        
        videoRef.current.load();
        
        let playAttempts = 0;
        const maxPlayAttempts = 5;
        
        const tryPlay = async (): Promise<boolean> => {
          try {
            addLog(`ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ù¾Ø®Ø´ ÙˆÛŒØ¯ÛŒÙˆ (ØªÙ„Ø§Ø´ ${playAttempts + 1})...`);
            await videoRef.current!.play();
            addLog('âœ… ÙˆÛŒØ¯ÛŒÙˆ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ù¾Ø®Ø´ Ú©Ø±Ø¯');
            return true;
          } catch (e) {
            playAttempts++;
            addLog(`âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø®Ø´ ÙˆÛŒØ¯ÛŒÙˆ (ØªÙ„Ø§Ø´ ${playAttempts}): ${e}`);
            
            if (playAttempts < maxPlayAttempts) {
              addLog('Ø¯Ø± Ø­Ø§Ù„ Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯...');
              await new Promise(resolve => setTimeout(resolve, 500));
              return tryPlay();
            }
            return false;
          }
        };

        const playSuccess = await tryPlay();
        
        if (!playSuccess) {
          addLog('âŒ ØªÙ…Ø§Ù… ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø®Ø´ ÙˆÛŒØ¯ÛŒÙˆ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯');
          setCameraError(true);
          setIsInitialized(false);
          return;
        }
        
        addLog('Ø¯Ø± Ø­Ø§Ù„ Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù† ÙˆÛŒØ¯ÛŒÙˆ...');
        const ready = await waitForVideoReady(15000);
        
        if (ready) {
          addLog('âœ… ÙˆÛŒØ¯ÛŒÙˆ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª');
        } else {
          addLog('âŒ ÙˆÛŒØ¯ÛŒÙˆ Ù¾Ø³ Ø§Ø² Ø§Ù†ØªØ¸Ø§Ø± Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù†Ø´Ø¯');
        }
      }
      
      setIsInitialized(true);
      addLog('âœ… Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯');

      const detectIntervalMs = 1000 / detectFps;
      let lastRun = performance.now();
      let consecutiveFailures = 0;
      const maxConsecutiveFailures = 10;

      const loop = async () => {
        rafIdRef.current = requestAnimationFrame(loop);

        const now = performance.now();
        if (now - lastRun < detectIntervalMs || detectionInProgressRef.current) return;
        detectionInProgressRef.current = true;
        const t0 = performance.now();

        try {
          drawFrameToCanvas();

          const result = await detectOnce();
          
          if (result.faceCount > 0 || result.violationType === 'no_face') {
            consecutiveFailures = 0;
          }
          
          setCurrentFaceCount(result.faceCount);

          if (isMonitoringRef.current) {
            const hasViolation = result.violationType !== null;
            setIsViolating(hasViolation);
            if (hasViolation) {
              const screenshot = captureScreenshot();
              const violation: Violation = {
                timestamp: new Date(),
                faceCount: result.faceCount,
                screenshot,
                type: result.violationType!,
                ...(result.distance !== undefined ? { distance: result.distance } : {}),
              };
              setViolations(prev => [...prev, violation]);
              addLog(`ğŸš¨ ØªØ®Ù„Ù Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯: ${result.violationType}`);
            }
          } else {
            setIsViolating(false);
          }
        } catch (e) {
          consecutiveFailures++;
          addLog(`âŒ Ø®Ø·Ø§ Ø¯Ø± Ø­Ù„Ù‚Ù‡ ØªØ´Ø®ÛŒØµ (ØªÙ„Ø§Ø´ ${consecutiveFailures}): ${e}`);
          
          if (consecutiveFailures >= maxConsecutiveFailures) {
            addLog('âŒ ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ Ø²ÛŒØ§Ø¯ Ø´Ø¯ØŒ Ø¯Ø± Ø­Ø§Ù„ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯ÙˆØ±Ø¨ÛŒÙ†...');
            consecutiveFailures = 0;
            
            try {
              stopCamera();
              await new Promise(resolve => setTimeout(resolve, 1000));
              await startCamera();
            } catch (restartError) {
              addLog('âŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯ÙˆØ±Ø¨ÛŒÙ† Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: ' + restartError);
            }
          }
        } finally {
          const t1 = performance.now();
          setFps(Math.round(1000 / Math.max(t1 - t0, 1)));
          detectionInProgressRef.current = false;
          lastRun = now;
        }
      };

      if (!rafIdRef.current) {
        rafIdRef.current = requestAnimationFrame(loop);
        addLog('âœ… Ø­Ù„Ù‚Ù‡ ØªØ´Ø®ÛŒØµ Ø´Ø±ÙˆØ¹ Ø´Ø¯');
      }
    } catch (err) {
      addLog('âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÙˆØ±Ø¨ÛŒÙ†: ' + err);
      setCameraError(true);
      setIsInitialized(false);
      throw err;
    }
  }, [detectOnce, captureScreenshot, drawFrameToCanvas, loadModels, isModelsLoaded, detectFps, waitForVideoReady]);

  const stopCamera = useCallback(() => {
    setIsInitialized(false);
    setCurrentFaceCount(0);
    setIsViolating(false);

    if (rafIdRef.current) {
      cancelAnimationFrame(rafIdRef.current);
      rafIdRef.current = null;
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }

    if (videoRef.current) {
      (videoRef.current as any).srcObject = null;
    }

    const overlay = overlayCanvasRef.current;
    if (overlay) {
      const ctx = overlay.getContext('2d');
      ctx?.clearRect(0, 0, overlay.width, overlay.height);
    }
  }, []);

  useEffect(() => {
    loadModels();
  }, [loadModels]);

  useEffect(() => {
    if (enabled) {
      startCamera().catch(() => {});
    } else {
      stopCamera();
    }
    return () => {
      stopCamera();
    };
  }, [enabled, startCamera, stopCamera]);

  useEffect(() => {
    return () => {
      stopCamera();
    };
  }, [stopCamera]);

  return {
    videoRef,
    canvasRef,
    overlayCanvasRef,
    isInitialized,
    currentFaceCount,
    isViolating,
    violations,
    referenceFace,
    isModelsLoaded,
    isMonitoring,
    cameraError,
    lastDistance,
    fps,
    startCamera,
    stopCamera,
    captureReferenceFace,
    startMonitoring,
  };
};